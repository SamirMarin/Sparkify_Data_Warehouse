This is an ETL pipeline that creates staging and analytic star schema tables on Redshift, extracts data hosted on S3 into the staging tables on Redshift, then runs sql queries to insert data from the staging tables into the star schema tables on Redshift

## The Datasets
there are two datasets both in json format

### Song Dataset
a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).
Containing the following columns:
> "num_songs", "artist_id", "artist_latitude", "artist_longitude", "artist_location", "artist_name", "song_id", "title", "duration", "year"

### Log Dataset
datset generated by [event simulator](https://github.com/Interana/eventsim) based on the songs in the song dataset.
Containing the following columns:
> "artist", "auth", "firstName", "gender", "itemInSession", "lastName", "length", "level", "location", "method", "page", "registration", "sessionId", "song", "status", "ts", "userAgent", "userId"

## The Staging Tables
an exact replica of the two datasets. Loaded using the Redshift copy query command to load data from s3 to Redshift.

### staging_events staging table
contains all data from the Logdata dataset, table contains all columns found in dataset

### staging_songs staging table
contains all data from the Song dataset, table contains all columns found in dataset

## Star Schema Analytical Tables

### Fact table
**songplays** - created using staging_events and staging_songs data
> songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension tables

1. **users** - created using staging_events data
> user_id, first_name, last_name, gender, level

2. **songs** - created using staging_songs data
> song_id, title, artist_id, year, duration

3. **artists** - created using staging_songs data
> artist_id, name, location, lattitude, longitude

4. **time** - crated using staging_events data
> start_time, hour, day, week, month, year, weekday
