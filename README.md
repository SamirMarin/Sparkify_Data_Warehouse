This is an ETL pipeline that creates staging and analytic star schema tables on Redshift, extracts data hosted on S3 into the staging tables on Redshift, then runs sql queries to insert data from the staging tables into the star schema tables on Redshift

## The Datasets
there are two datasets both in json format

### Song Dataset
a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).
Containing the following columns:
> "num_songs", "artist_id", "artist_latitude", "artist_longitude", "artist_location", "artist_name", "song_id", "title", "duration", "year"

### Log Dataset
datset generated by [event simulator](https://github.com/Interana/eventsim) based on the songs in the song dataset.
Containing the following columns:
> "artist", "auth", "firstName", "gender", "itemInSession", "lastName", "length", "level", "location", "method", "page", "registration", "sessionId", "song", "status", "ts", "userAgent", "userId"

## The Staging Tables
an exact replica of the two datasets. Loaded using the Redshift copy query command to load data from s3 to Redshift.

### staging_events staging table
contains all data from the Logdata dataset, table contains all columns found in dataset

### staging_songs staging table
contains all data from the Song dataset, table contains all columns found in dataset

## Star Schema Analytical Tables

### Fact table
**songplays** - created using staging_events and staging_songs data
> songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension tables

1. **users** - created using staging_events data
> user_id, first_name, last_name, gender, level

2. **songs** - created using staging_songs data
> song_id, title, artist_id, year, duration

3. **artists** - created using staging_songs data
> artist_id, name, location, lattitude, longitude

4. **time** - crated using staging_events data
> start_time, hour, day, week, month, year, weekday

## The ETL Pipeline
The etl pipeline consit of two python scripts: **create_tables.py** and **etl.py**.

Both tables depend on the dwh.cfg file. This file contains information related to the AWS. Specifically three pieces of information: **cluster**, **role**, and **s3**

1. **Cluster** - This is the AWS redshift cluster information. This is blank by default as it must be filled out of by the person running the etl pipeline. This is related to the AWS redshift cluster that will be used to host the tables. To obtain this information you must create a redshift database on AWS.

2. **Role** - This is the AWS IAM Role. This is blank by default as it must be filled out of by the person running the etl pipeline. This is needed to enable your Redshift cluster to load data from Amazon S3 buckets. Your Redshift cluster must be crated with this role. Thus the Role must be created before the cluster.

3. **S3** - This is the location of the data in the S3 buckets. This is filled and can be left as is.

One more thing that is needed in order to be able to access your redshift cluster from your machine is an AWS Security Group. This must also be used when creating your redshift cluster. Using the default security group will not allow you to access the redshift cluster with your machine.


